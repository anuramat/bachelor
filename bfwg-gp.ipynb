{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EEy0ioPVVo0"
   },
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1617469940425,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "LoMpnMT6lyWW"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "clear = lambda: clear_output(wait=True)\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "import torchvision.transforms\n",
    "from io import BytesIO\n",
    "writer = SummaryWriter('runs/bfwg-gp')\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RchZGvjzViFx"
   },
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 5055,
     "status": "ok",
     "timestamp": 1617469944273,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "Vx3om4aja2I0",
    "outputId": "92841465-5f9f-4fea-9ff5-e2a5a3d9fe5b"
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv('beamfile.txt', sep=':')\n",
    "print('Total number of points in the dataset:', full_df.size)\n",
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 5047,
     "status": "ok",
     "timestamp": 1617469944273,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "j67aUMUya5L2",
    "outputId": "b9f53655-c601-44e2-a92b-49de31743cee"
   },
   "outputs": [],
   "source": [
    "df = full_df[full_df['particleFlag']==2].drop(columns = ['stepNumber', 'particleFlag', 'Z']) # only core\n",
    "# df = full_df.drop(columns = ['stepNumber', 'particleFlag', 'Z']) # all points\n",
    "print('Number of points from beam core (i.e. flag=2):', df.size)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5041,
     "status": "ok",
     "timestamp": 1617469944274,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "t4legbhHTXbw"
   },
   "outputs": [],
   "source": [
    "real_data_raw = df.to_numpy()\n",
    "scaler = preprocessing.StandardScaler().fit(real_data_raw)\n",
    "real_data = scaler.transform(real_data_raw)\n",
    "dataset = torch.utils.data.TensorDataset(torch.FloatTensor(real_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSyqIh6tVX0L"
   },
   "source": [
    "# Define generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 5 # number of variables in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 1247,
     "status": "ok",
     "timestamp": 1617469940426,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "Bmb3L6E2RKj3"
   },
   "outputs": [],
   "source": [
    "class SequentialLinearModule(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, layers, activation, out_activation, bias):\n",
    "        \n",
    "        '''\n",
    "        if layers is list: list containing the size of the linear layers between input and output\n",
    "        if layers is int: number of layers with exponentially decreasing size\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        assert type(layers) is list or type(layers) is int, '\"layers\" argument should be list or str, more in doctstring'\n",
    "\n",
    "        if type(layers) is list:\n",
    "            layer_sizes = layers\n",
    "            layers.insert(0, in_dim)\n",
    "            num_layers = len(layer_sizes)\n",
    "\n",
    "        if type(layers) is int:\n",
    "            layer_sizes = []\n",
    "            num_layers = layers\n",
    "            # exponential decrease\n",
    "            for i in range(num_layers):\n",
    "                layer_sizes.append(int(in_dim/(2**i)))\n",
    "\n",
    "        # stack the layers\n",
    "        layer_sizes.append(out_dim)\n",
    "        layers_list = []\n",
    "        for i in range(num_layers):\n",
    "            if i!=0:\n",
    "                layers_list.append(activation())\n",
    "            layers_list.append(nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=bias))\n",
    "        layers_list.append(out_activation())\n",
    "        self.f = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1617469940427,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "S8F6WLaZFKXu"
   },
   "outputs": [],
   "source": [
    "class Generator(SequentialLinearModule):\n",
    "    def __init__(self, noise_dim, n_dim, layers):\n",
    "        super().__init__(noise_dim, n_dim, layers, nn.ReLU, nn.Identity, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1617469940428,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "S86adVJdJHDQ"
   },
   "outputs": [],
   "source": [
    "class Critic(SequentialLinearModule):\n",
    "    def __init__(self, n_dim, layers):\n",
    "        super().__init__(n_dim, 1, layers, nn.LeakyReLU, nn.LeakyReLU, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQIY8q9iVc3s"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1231,
     "status": "ok",
     "timestamp": 1617469940428,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "aTDvLLuyy8j9",
    "outputId": "839426fe-17a0-4135-8bc8-7b78fb28775c"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "noise_dim = 128\n",
    "batch_size = 512\n",
    "batch_progress_period = int( len(dataset)/batch_size * 0.1 )\n",
    "num_epochs = 50\n",
    "lr_critic = lr\n",
    "lr_gen = lr/4\n",
    "\n",
    "critic_iterations = 5\n",
    "lambda_gp = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-X1vybfVgCD"
   },
   "source": [
    "# Initialize main objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 4699,
     "status": "ok",
     "timestamp": 1617469943902,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "g9sV_O8KqRy3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "critic = Critic(n_dim, [32 for _ in range(2)]).to(device)\n",
    "gen = Generator(noise_dim, n_dim, [32 for _ in range(3)]).to(device)\n",
    "critic_loss_list = []\n",
    "gen_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(critic)\n",
    "# print('\\n'*3)\n",
    "# print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4695,
     "status": "ok",
     "timestamp": 1617469943905,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "kefT5YWIRrJf",
    "outputId": "30e960be-1490-4c9e-8cfc-8b55577caaba"
   },
   "outputs": [],
   "source": [
    "opt_critic = optim.Adam(critic.parameters(), lr=lr_critic, betas=(0.0, 0.9))\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr_gen, betas=(0.0, 0.9))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGdWnsrQVkqU"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake):\n",
    "    batch_size, n_dim = real.shape\n",
    "    epsilon = torch.rand((batch_size, 1)).repeat(1, n_dim).to(device)\n",
    "    interpolated_images = real*epsilon + fake*(1-epsilon)\n",
    "\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0] # (batch_size, n_dim=5)\n",
    "\n",
    "    gradient_norm = gradient.norm(2, dim=1) # (batch_size)\n",
    "    \n",
    "    return torch.mean((gradient_norm - 1) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 563,
     "status": "error",
     "timestamp": 1617471271792,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "OYorpkoSPPpW",
    "outputId": "ccefabf6-c540-446a-d3b1-16f38993ce18"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    running_critic_loss = 0\n",
    "    running_gen_loss = 0\n",
    "    \n",
    "    last_time = None\n",
    "    eta = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # batch is stored in \"real\" variable (as opposed to \"fake\" generated data)\n",
    "        for batch_idx, (real, ) in enumerate(dataloader):\n",
    "\n",
    "            real = real.to(device)\n",
    "\n",
    "            for _ in range(critic_iterations):\n",
    "\n",
    "                # generate fake data\n",
    "                noise = torch.randn(real.shape[0], noise_dim).to(device) # noise for generator\n",
    "                fake = gen(noise) # fake data\n",
    "\n",
    "                # critic training\n",
    "                real_preds = critic(real)\n",
    "                fake_preds = critic(fake)\n",
    "                gp = gradient_penalty(critic, real, fake)\n",
    "                loss_critic = torch.mean(fake_preds) - torch.mean(real_preds) + lambda_gp*gp\n",
    "                critic.zero_grad()\n",
    "                loss_critic.backward(retain_graph=True) # without retain_graph all graph variables will be freed to optimize memory consumption\n",
    "                opt_critic.step()\n",
    "                running_critic_loss += loss_critic.detach().cpu().numpy()\n",
    "\n",
    "            # generator training\n",
    "            fake_preds = critic(fake)\n",
    "            loss_gen = -torch.mean(fake_preds)\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "            running_gen_loss += loss_gen.detach().cpu().numpy()\n",
    "            \n",
    "            if (batch_progress_period!=0 and batch_idx % batch_progress_period == 0) or batch_idx==0:\n",
    "\n",
    "                text = ''\n",
    "                # epoch num and %\n",
    "                text += f'Epoch {epoch+1}/{num_epochs}\\n'\n",
    "                text += f'{100*epoch/num_epochs:.2f}%\\n\\n'\n",
    "                # batch num and %\n",
    "                text += f'Batch {batch_idx+1}/{len(dataloader)}\\n'\n",
    "                text += str(100*batch_idx/len(dataloader))[:5] + '%'\n",
    "                text += f'{100*batch_idx/len(dataloader):.2f}%\\n\\n'\n",
    "        \n",
    "                # eta\n",
    "                if last_time and batch_progress_period!=0:\n",
    "                    eta = (((len(dataloader)-batch_idx) + (num_epochs-epoch-1)*len(dataloader))/batch_progress_period*(time.time()-last_time))\n",
    "                last_time = time.time()\n",
    "                if type(eta) is float:\n",
    "                    eta_string = str(datetime.timedelta(seconds=eta))\n",
    "                    text += f\"ETA {':'.join([i.split('.')[0] for i in eta_string.split(':')])}\\n\"\n",
    "                else:\n",
    "                    text += 'ETA N/A\\n'\n",
    "                \n",
    "                writer.add_text('progress', text, epoch*len(dataloader) + batch_idx)\n",
    "                \n",
    "                img_buf = draw_hist()\n",
    "                img = Image.open(img_buf)\n",
    "                img = torchvision.transforms.ToTensor()(img)\n",
    "                writer.add_image('histograms', img, epoch*len(dataloader) + batch_idx)\n",
    "                \n",
    "                # memory usage\n",
    "                writer.add_scalar('memory', torch.cuda.max_memory_allocated()/1024**3, epoch*len(dataloader) + batch_idx)\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                # loss plots\n",
    "                running_critic_loss /= batch_progress_period*critic_iterations\n",
    "                running_gen_loss /= batch_progress_period\n",
    "    \n",
    "                writer.add_scalar('loss/critic', running_critic_loss, epoch*len(dataloader) + batch_idx)\n",
    "                writer.add_scalar('loss/generator', running_gen_loss, epoch*len(dataloader) + batch_idx)\n",
    "                \n",
    "                running_critic_loss = 0\n",
    "                running_gen_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5395,
     "status": "ok",
     "timestamp": 1617470072286,
     "user": {
      "displayName": "Xhdhdh Cidjej",
      "photoUrl": "",
      "userId": "04686642470648718966"
     },
     "user_tz": -180
    },
    "id": "-lFtipAPnWiK",
    "outputId": "1b07b544-2866-479a-e5ce-1f11190af595",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_hist():\n",
    "    \n",
    "    noise = torch.randn(real_data.shape[0], noise_dim).to(device)\n",
    "    fake = gen(noise).detach().cpu().numpy()\n",
    "    real = real_data\n",
    "    bins = 100\n",
    "        \n",
    "    plotnum = 1\n",
    "    fig, ax = plt.subplots(figsize=(14, 70)) # size in inches\n",
    "    for i in range(5):\n",
    "        for j in range(i+1, 5):\n",
    "            # (i,j) are the columns that we are going to draw a histogram of\n",
    "\n",
    "            # real data\n",
    "            plt.subplot(10, 2, plotnum)\n",
    "            plt.title(f'real, {df.columns[i]}/{df.columns[j]}')\n",
    "            plotnum += 1\n",
    "            _, xedges, yedges, _ = plt.hist2d(real[:,i], real[:,j], bins=bins,)# range = [[x_min,x_max], [y_min,y_max]])\n",
    "            \n",
    "            x_min = xedges[0]\n",
    "            x_max = xedges[-1]\n",
    "            y_min = yedges[0]\n",
    "            y_max = yedges[-1]\n",
    "\n",
    "            # fake data\n",
    "            plt.subplot(10, 2, plotnum)\n",
    "            plt.title(f'fake, {df.columns[i]}/{df.columns[j]}')\n",
    "            plotnum += 1\n",
    "            plt.hist2d(fake[:,i], fake[:,j], bins=bins, range = [[x_min,x_max], [y_min,y_max]])\n",
    "            \n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-b2a65d6a222d>:9: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(figsize=(14, 70)) # size in inches\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNBaRV3NAD7Z6IdingWfkOz",
   "collapsed_sections": [],
   "mount_file_id": "1a7rDIvb294KRdEVYVXYfqYJMa1YeQjAR",
   "name": "big_fysics_gan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
